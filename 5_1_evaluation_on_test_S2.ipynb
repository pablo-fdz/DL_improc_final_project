{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0598af90",
   "metadata": {},
   "source": [
    "Evaluating the model trained with **Sentinel-2 L2A** data (natural-looking images)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46647971",
   "metadata": {},
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884fdfe2",
   "metadata": {},
   "source": [
    "## 0.1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a7920e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Seed set to 42 for NumPy, Torch and Random for reproducibility.\n"
     ]
    }
   ],
   "source": [
    "# Utilities\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "from torch import nn, utils\n",
    "from torchmetrics import classification\n",
    "\n",
    "# Custom library\n",
    "from library import nn_model, utilities, visualizations, preprocessing\n",
    "\n",
    "# Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device = torch.device(device)\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Utilities\n",
    "# classes = {0:'non-building', 255:'building'}\n",
    "seed = 42  # Set the same seed as for the corresponding training run with S2-L2A data!\n",
    "utilities.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e47ead",
   "metadata": {},
   "source": [
    "## 0.2. Setting the path to the data\n",
    "\n",
    "We set the path to the tiles that have been created in the preprocessing notebook, `3_preprocessing.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f599fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_input_dir = '/media/pablo/Shared files/data/'  # Adjust this path to your data directory containing the labelled dataset\n",
    "input_labelled_dir = os.path.join(base_input_dir, 'Satellite_burned_area_dataset')  # Path to the original labelled dataset\n",
    "tile_dir = os.path.join(base_input_dir, 'tiled_labelled_dataset')  # Path to the tiled dataset\n",
    "sentinel_type = 2  # Sentinel-2 data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9796b8",
   "metadata": {},
   "source": [
    "## 0.3. Notebook description\n",
    "\n",
    "In this notebook we evaluate the model trained on Sentinel-2 L2A images on the test split made in that notebook.\n",
    "\n",
    "> For consistency, in this notebook we again create the same split in the same way, but in this case we will only be using the test data. **Please ensure that the seeds set in both notebooks are the same for avoiding data leakage and creating the same test set in both notebooks**. Furthermore, pre-processing must be implemented in the same way as for the training notebook with Sentinel-2 L2A images. It is done here again for clarity.\n",
    "\n",
    "*Reminder of label interpretation: **the brighter the label in the mask, the higher the severity of the fire (white if maximum severity)**. Areas affected by fires of lower severity are darker. There are 5 severity levels, from 0 (not burned) to 4 (maximum burn damage).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d2bfca",
   "metadata": {},
   "source": [
    "# 1. Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1a29c6",
   "metadata": {},
   "source": [
    "## 1.1. Getting the image and masks paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9655dad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 fire folders:\n",
      "['EMSR207_01MIRANDADOCORVO_02GRADING_MAP_v2_vector', 'EMSR207_02LOUSA_02GRADING_MAP_v2_vector', 'EMSR207_03PAMPILHOSADASERRA_02GRADING_MAP_v2_vector', 'EMSR207_04AVELAR_02GRADING_MAP_v2_vector', 'EMSR207_05PEDROGAOGRANDE_02GRADING_MAP_v2_vector']\n",
      "Total number of fire folders: 73\n",
      "Fire folder: EMSR207_01MIRANDADOCORVO_02GRADING_MAP_v2_vector\n",
      "Fire folder: EMSR207_02LOUSA_02GRADING_MAP_v2_vector\n",
      "Fire folder: EMSR207_03PAMPILHOSADASERRA_02GRADING_MAP_v2_vector\n",
      "Fire folder: EMSR207_04AVELAR_02GRADING_MAP_v2_vector\n",
      "Fire folder: EMSR207_05PEDROGAOGRANDE_02GRADING_MAP_v2_vector\n",
      "Fire folder: EMSR207_06MADEIRA_02GRADING_MAP_v2_vector\n",
      "Fire folder: EMSR207_07ALVAIAZERE_02GRADING_MAP_v2_vector\n",
      "Fire folder: EMSR207_08CERNACHEDOBONJARDIM_02GRADING_MAP_v2_vector\n",
      "Fire folder: EMSR207_10ORVALHO_02GRADING_MAP_v2_vector\n",
      "Fire folder: EMSR209_01MOGUER_02GRADING_MAP_v2_vector\n",
      "Fire folder: EMSR209_02MAZAGON_02GRADING_MAP_v2_vector\n",
      "Fire folder: EMSR209_03LOSCABEZUDOS_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR209_04CHOZASDELAPOLEOSA_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR209_05MAZAGONDETAIL01_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR210_03ELCAMPILLO_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR210_04TRASLASIERRA_02GRADING_MAP_v2_vector\n",
      "Fire folder: EMSR210_05ELCAMPILLOOVERVIEW_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR211_01SONEJAWEST_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR211_02SONEJA_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR213_01VESUVIO_02GRADING_MAP_v2_vector\n",
      "Fire folder: EMSR213_03FRANCOFONTE_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR213_04AVOLA_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR213_06MONREALE_02GRADING_MAP_v2_vector\n",
      "Fire folder: EMSR213_07SANVITOLOCAPO_02GRADING_MAP_v2_vector\n",
      "Fire folder: EMSR213_08BLUFI_02GRADING_MAP_v2_vector\n",
      "Fire folder: EMSR213_09MISTRETTA_02GRADING_MAP_v2_vector\n",
      "Fire folder: EMSR213_10MESSINA_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR213_11NASO_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR213_12ETNANORD_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR214_01OLMETA_02GRADING_MAP_v2_vector\n",
      "Fire folder: EMSR214_02LABASTIDONNE_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR214_04MARSEILLE_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR214_05LELAVANDOU_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR214_06COGOLIN_02GRADING_MAP_v2_vector\n",
      "Fire folder: EMSR216_01LOSCOLLADOS_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR216_02TORREPEDRO_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR216_04RALA_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR216_05ELCALAR_02GRADING_MAP_v2_vector\n",
      "Fire folder: EMSR217_01DUKAT_02GRADING_MAP_v2_vector\n",
      "Fire folder: EMSR217_02VRANISHT_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR217_03SLLATINE_02GRADING_MAP_v2_vector\n",
      "Fire folder: EMSR217_04DAJTI_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR219_01SEGURADELASIERRA_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR221_01OGLIASTRO_02GRADING_MAP_v2_vector\n",
      "Fire folder: EMSR226_01DABA_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR227_01ENCINEDO_02GRADING_MAP_v2_vector\n",
      "Fire folder: EMSR227_02CORPORALES_02GRADING_MAP_v2_vector\n",
      "Fire folder: EMSR227_03SANTAEULALIADECABRERA_02GRADING_MAP_v2_vector\n",
      "Fire folder: EMSR227_04TRUCHILLAS_02GRADING_MAP_v2_vector\n",
      "Fire folder: EMSR237_03ELMADRONO_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR237_04ELCASTILLODELASGUARDAS_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR239_05ALMADENDELAPLATAOVERVIEW_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR248_01PINODELORO_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR248_04HOYOSDEMIGUELMUNOZ_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR250_01MARINHAGRANDE_02GRADING_MAP_v2_vector\n",
      "Fire folder: EMSR250_04CORTES_02GRADING_MAP_v2_vector\n",
      "Fire folder: EMSR252_02HAUTECORSEDETAIL_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR254_01PORTELADEAGUIAR_02GRADING_MAP_v2_vector\n",
      "Fire folder: EMSR254_02SILVAN_02GRADING_MAP_v2_vector\n",
      "Fire folder: EMSR254_03PERENADELARIBERA_02GRADING_MAP_v2_vector\n",
      "Fire folder: EMSR281_01KIRTOMY_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR290_03MANSBO_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR298_02HAMMARSTRAND_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR298_05STRANDASMYRVALLEN_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR298_06GROTINGEN_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR302_01NERVA_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR302_06NERVADETAIL_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR302_07ELPERALEJO_02GRADING_MAP_v1_vector\n",
      "Fire folder: EMSR365_AOI01_GRA_PRODUCT_r1_RTP01_v1_vector\n",
      "Fire folder: EMSR368_AOI01_GRA_PRODUCT_r1_RTP01_v3_vector\n",
      "Fire folder: EMSR371_AOI01_GRA_PRODUCT_r1_RTP01_v2_vector\n",
      "Fire folder: EMSR372_AOI04_GRA_PRODUCT_r1_RTP01_v3_vector\n",
      "Fire folder: EMSR373_AOI01_GRA_PRODUCT_r1_RTP01_v2_vector\n",
      "First 5 image and label paths of the fire folder: EMSR207_01MIRANDADOCORVO_02GRADING_MAP_v2_vector\n",
      "['/media/pablo/Shared files/data/tiled_labelled_dataset/EMSR207_01MIRANDADOCORVO_02GRADING_MAP_v2_vector/images/sentinel2_2017-07-04_tile_0_0.tiff', '/media/pablo/Shared files/data/tiled_labelled_dataset/EMSR207_01MIRANDADOCORVO_02GRADING_MAP_v2_vector/images/sentinel2_2017-07-04_tile_0_1.tiff', '/media/pablo/Shared files/data/tiled_labelled_dataset/EMSR207_01MIRANDADOCORVO_02GRADING_MAP_v2_vector/images/sentinel2_2017-07-04_tile_0_2.tiff', '/media/pablo/Shared files/data/tiled_labelled_dataset/EMSR207_01MIRANDADOCORVO_02GRADING_MAP_v2_vector/images/sentinel2_2017-07-04_tile_0_3.tiff', '/media/pablo/Shared files/data/tiled_labelled_dataset/EMSR207_01MIRANDADOCORVO_02GRADING_MAP_v2_vector/images/sentinel2_2017-07-04_tile_0_4.tiff']\n",
      "['/media/pablo/Shared files/data/tiled_labelled_dataset/EMSR207_01MIRANDADOCORVO_02GRADING_MAP_v2_vector/masks/EMSR207_01MIRANDADOCORVO_02GRADING_MAP_v2_vector_mask_tile_0_0.tiff', '/media/pablo/Shared files/data/tiled_labelled_dataset/EMSR207_01MIRANDADOCORVO_02GRADING_MAP_v2_vector/masks/EMSR207_01MIRANDADOCORVO_02GRADING_MAP_v2_vector_mask_tile_0_1.tiff', '/media/pablo/Shared files/data/tiled_labelled_dataset/EMSR207_01MIRANDADOCORVO_02GRADING_MAP_v2_vector/masks/EMSR207_01MIRANDADOCORVO_02GRADING_MAP_v2_vector_mask_tile_0_2.tiff', '/media/pablo/Shared files/data/tiled_labelled_dataset/EMSR207_01MIRANDADOCORVO_02GRADING_MAP_v2_vector/masks/EMSR207_01MIRANDADOCORVO_02GRADING_MAP_v2_vector_mask_tile_0_3.tiff', '/media/pablo/Shared files/data/tiled_labelled_dataset/EMSR207_01MIRANDADOCORVO_02GRADING_MAP_v2_vector/masks/EMSR207_01MIRANDADOCORVO_02GRADING_MAP_v2_vector_mask_tile_0_4.tiff']\n",
      "Total number of fire folders in image_paths: 73\n",
      "Total number of fire folders in label_paths: 73\n"
     ]
    }
   ],
   "source": [
    "# Get all of the folders within the tiles directory\n",
    "fire_folders = sorted([f for f in os.listdir(tile_dir) if os.path.isdir(os.path.join(tile_dir, f))])\n",
    "# Print the first 5 folders\n",
    "print(\"First 5 fire folders:\")\n",
    "print(fire_folders[:5])\n",
    "# Print the total number of fire folders\n",
    "print(\"Total number of fire folders:\", len(fire_folders))\n",
    "\n",
    "# Get the paths for the images and the labels\n",
    "image_paths = []\n",
    "label_paths = []\n",
    "for fire_folder in fire_folders:\n",
    "    print(f\"Fire folder: {fire_folder}\")\n",
    "    images_path = os.path.join(tile_dir, fire_folder, 'images')\n",
    "    labels_path = os.path.join(tile_dir, fire_folder, 'masks')\n",
    "    if os.path.exists(images_path):\n",
    "        image_paths.append(sorted([os.path.join(images_path, img) for img in os.listdir(images_path) if img.startswith(f'sentinel{sentinel_type}') and img.endswith('.tiff')]))\n",
    "    else:\n",
    "        print(f\"Images path does not exist: {images_path}\")\n",
    "    if os.path.exists(labels_path):\n",
    "        label_paths.append(sorted([os.path.join(labels_path, lbl) for lbl in os.listdir(labels_path) if lbl.endswith('.tiff')]))\n",
    "    else:\n",
    "        print(f\"Labels path does not exist: {labels_path}\")\n",
    "# Print the first 5 image paths\n",
    "print(f\"First 5 image and label paths of the fire folder: {fire_folders[0]}\")\n",
    "print(image_paths[0][:5])  # The result is a list of lists, where each sublist contains the paths of images for a specific fire folder\n",
    "print(label_paths[0][:5])  # The result is a list of lists, where each sublist contains the paths of labels for a specific fire folder\n",
    "\n",
    "# Print the total number of lists in image_paths and label_paths (should be equal to the number of fire folders)\n",
    "print(\"Total number of fire folders in image_paths:\", len(image_paths))\n",
    "print(\"Total number of fire folders in label_paths:\", len(label_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cd6983",
   "metadata": {},
   "source": [
    "## 1.2. Loading the image and masks into tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7b7470",
   "metadata": {},
   "source": [
    "We load the images and labels as `torch.Tensor` of dimensions $n \\times d \\times h \\times w$, where:\n",
    "- $n$: number of images (tiles)/masks.\n",
    "- $d$: number of channels for the images/masks.\n",
    "- $h$: height of the images and of the masks. Both are of 256 height, as designed in `3_preprocessing`.\n",
    "- $w$: width of the images and of the masks. Both are of 256 height, as designed in `3_preprocessing`.\n",
    "\n",
    "We normalize each channel to $N[0,1]$ and set the data type as `torch.float32` (set to `torch.float16` for greater memory efficiency). Why we normalize image channels:\n",
    "\n",
    "1. **Gradient stability**: Different channels may have vastly different value ranges, leading to unstable gradients\n",
    "2. **Training speed**: Normalized inputs help the optimizer converge faster\n",
    "3. **Weight initialization**: Most weight initialization schemes assume normalized inputs\n",
    "4. **Activation functions**: Work optimally with inputs in specific ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc80c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the lists of image and label paths\n",
    "images_flat = [item for sublist in image_paths for item in sublist]\n",
    "labels_flat = [item for sublist in label_paths for item in sublist]\n",
    "\n",
    "# Read images and labels and stack them into a single tensor (reduce precision to float16 for greater efficiency)\n",
    "images = torch.stack([utilities.read_tiff_to_torch(file_path = image, dtype=torch.float32, normalize=True, normalization_method='per_channel') for image in images_flat])\n",
    "labels_raw = torch.stack([utilities.read_tiff_to_torch(file_path = label, dtype = torch.float32, normalize=True, normalization_method='255') for label in labels_flat])  # Masks are encoded in 0-255 range according to the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41166c6a",
   "metadata": {},
   "source": [
    "Now, let's check the array dimensions and display several image–label pairs to ensure that they are correctly aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7fda3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the image tensor: torch.Size([3230, 13, 256, 256])\n",
      "Size of the label tensor: torch.Size([3230, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "print('Size of the image tensor:', images.size())\n",
    "print('Size of the label tensor:', labels_raw.size())\n",
    "\n",
    "# Check if the number of images and labels are equal\n",
    "if images.size(0) != labels_raw.size(0):\n",
    "    raise ValueError('Number of images and labels do not match!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa56c1a6",
   "metadata": {},
   "source": [
    "i.e., in this case:\n",
    "\n",
    "- $n$: we have 3,230 images and masks (3,374 images before removing those without coverage).\n",
    "- $d$: number of channels for the images/masks. The images have 13 dimensions (bands B01-B12 from Sentinel-2 L2A, https://documentation.dataspace.copernicus.eu/APIs/SentinelHub/Data/S2L2A.html#available-bands-and-data).\n",
    "- $h$: height of the images and of the masks. Both are of 256 height, as designed in `3_preprocessing`.\n",
    "- $w$: width of the images and of the masks. Both are of 256 height, as designed in `3_preprocessing`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17c81f3",
   "metadata": {},
   "source": [
    "## 1.3. Mapping the classes to integers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d41625a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the mapping\n",
    "labels = utilities.map_labels_to_classes_approximate(labels_raw)\n",
    "\n",
    "del labels_raw  # Free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965d4bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in the labels tensor after mapping: tensor([0, 1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Print the unique values in the labels tensor to verify the mapping\n",
    "print(\"Unique values in the labels tensor after mapping:\", torch.unique(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230a0056",
   "metadata": {},
   "source": [
    "## 1.4. Formatting the data as a `TensorDataset`\n",
    "\n",
    "Useful documentation: [`torch.utils.data`](https://pytorch.org/docs/stable/data.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e740458c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3230\n"
     ]
    }
   ],
   "source": [
    "# Format the data as a TensorDataset\n",
    "dataset = utils.data.TensorDataset(images, labels)  # Create a TensorDataset from the image and label tensors\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bf39cd",
   "metadata": {},
   "source": [
    "## 1.5. Splitting data into train, validation and test\n",
    "\n",
    "We split into 70% training data, 15% validation and 15% test.\n",
    "\n",
    "> Ensure that the seed is set to the same number as the model that is going to be used for prediction (check in the filename)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8f7aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation and test sets\n",
    "train_dataset, val_dataset, test_dataset = utils.data.random_split(dataset, [0.7, 0.15, 0.15], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Set number of physical CPU cores (as int) to use for data loading\n",
    "num_workers = os.cpu_count() // 2 if os.cpu_count() is not None else 0\n",
    "\n",
    "# Create DataLoader just for the test dataset\n",
    "test_loader = utils.data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,  # Keep test data in the same order for consistent evaluation\n",
    "    num_workers=num_workers,  # Use multiple workers for faster data loading\n",
    "    pin_memory=True if device == 'cuda' else False,  # Pin memory for faster data transfer to GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5ae1bb",
   "metadata": {},
   "source": [
    "# 2. Error analysis with the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d31552",
   "metadata": {},
   "source": [
    "## 2.1. Loading a saved PyTorch model\n",
    "\n",
    "Check the `models` directory within this repo. In principle, opt for the models with the lowest validation loss.\n",
    "\n",
    "To load a save model, we need to:\n",
    "1. Initialize a new model with the same architecture\n",
    "2. Load the state dictionary from the saved file (in the section \"Estimating the model weights\", this is done in the line `torch.save(model.state_dict(), [...])`)\n",
    "3. Apply the state dictionary to the model\n",
    "\n",
    "All of these steps are automatically implemented with the `load_model` function of the `library`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3d0281",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'unet_sentinel2_seed42_epochs_200_val_loss_0.9021.pth'\n",
    "\n",
    "model = utilities.load_model(os.path.join('models', model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfe164b",
   "metadata": {},
   "source": [
    "## 2.2. Predict probabilities for test images\n",
    "\n",
    "For more information, check the `predict` function in the `library`. It follows the same logic as the `train_model` function, but is optimized for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f1aaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "test_predictions, test_labels = nn_model.predict(\n",
    "    model=model,\n",
    "    dataloader=test_loader,\n",
    "    device=device,\n",
    "    num_classes=5,  # Number of classes in the dataset (0-4 for none to high burn severity)\n",
    "    return_probs=False,  # Get predictions as class labels\n",
    ")\n",
    "\n",
    "print(f\"Predictions shape: {test_predictions.shape}\")\n",
    "print(f\"Labels shape: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f428de11",
   "metadata": {},
   "source": [
    "## 2.3. Visualizing sample images and their predictions\n",
    "\n",
    "For more information, check `visualize_predictions` in `library`.\n",
    "\n",
    "> THE `visualize_predictions` FUNCTION MUST BE ADAPTED TO THESE MULTI-CHANNEL IMAGES!!! TAKE `display_image` AS THE INSPIRATION (OR EVEN CALL IT WITHIN THE `visualize_predictions` FUNCTION). I have already made some changes that should make the function work, but I'm not 100% sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ef7cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of test images and their labels\n",
    "test_images_sample = []\n",
    "test_true_masks_sample = []\n",
    "\n",
    "# Extract a few batches from the test loader\n",
    "for images, labels in test_loader:\n",
    "    test_images_sample.append(images)\n",
    "    test_true_masks_sample.append(labels.squeeze(1))\n",
    "    if len(test_images_sample) >= 3:  # Get 3 batches at most\n",
    "        break\n",
    "\n",
    "# Concatenate batches\n",
    "test_images_sample = torch.cat(test_images_sample, dim=0)\n",
    "test_true_masks_sample = torch.cat(test_true_masks_sample, dim=0)\n",
    "\n",
    "# Make predictions (just for 3 batches)\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    # Get raw logits from the model [N, C, H, W]\n",
    "    test_pred_logits = model(test_images_sample.to(device)).cpu()\n",
    "    # Get predicted class for each pixel by taking the argmax along the class dimension\n",
    "    test_pred_masks_sample = torch.argmax(test_pred_logits, dim=1)\n",
    "\n",
    "# Visualize\n",
    "visualizations.visualize_predictions(\n",
    "    images=test_images_sample, \n",
    "    true_masks=test_true_masks_sample, \n",
    "    predictions=test_pred_masks_sample, \n",
    "    num_samples=3,\n",
    "    num_classes=5, # As defined in the notebook\n",
    "    rgb_bands=(3, 2, 1) # Sentinel-2 RGB bands are B4, B3, B2 which are indices 3, 2, 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7976e4bc",
   "metadata": {},
   "source": [
    "## 2.4. Classification metrics\n",
    "\n",
    "We use several classification metrics to assess the performance of the model, including accuracy, precision, recall, F1 and intersection over union (IoU), as well as the confusion metrics for multi-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34a1790",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bb981f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the predictions and labels to 1D arrays for confusion matrix calculation\n",
    "y_pred_flat = test_predictions.flatten()\n",
    "y_true_flat = test_labels.flatten()\n",
    "\n",
    "# Convert to numpy arrays if they're not already\n",
    "if torch.is_tensor(y_pred_flat):\n",
    "    y_pred_flat = y_pred_flat.cpu().numpy()\n",
    "    y_true_flat = y_true_flat.cpu().numpy()\n",
    "\n",
    "# Define class names for the plot\n",
    "class_names = ['Unburned', 'Low Sev.', 'Moderate-Low Sev.', 'Moderate-High Sev.', 'High Sev.']\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true_flat, y_pred_flat)\n",
    "\n",
    "# Visualization of confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for Burn Severity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e639d49",
   "metadata": {},
   "source": [
    "### Full classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f73478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IoU\n",
    "jaccard_metric = classification.MulticlassJaccardIndex().to(device)  # Initialize metrics\n",
    "# Move tensors to the same device as the metrics to avoid errors\n",
    "test_predictions_device = test_predictions.to(device)\n",
    "test_labels_device = test_labels.to(device)\n",
    "# Calculate metrics (make sure inputs are tensors)\n",
    "iou_torch = jaccard_metric(test_predictions_device, test_labels_device)\n",
    "\n",
    "# Define class names for the report\n",
    "class_names = ['Unburned (0)', 'Low Sev. (1)', 'Mod-Low Sev. (2)', 'Mod-High Sev. (3)', 'High Sev. (4)']\n",
    "\n",
    "print(classification_report(y_true_flat, y_pred_flat, \n",
    "                           target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b98d58",
   "metadata": {},
   "source": [
    "### Precision-recall for multi-class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac307f3b",
   "metadata": {},
   "source": [
    "### ROC curve for multi-class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1132d0d6",
   "metadata": {},
   "source": [
    "### Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1dc42d",
   "metadata": {},
   "source": [
    "## 2.5. Visualization of classification errors\n",
    "\n",
    "Let's visualize true positives, true negatives, false positives, and false negatives as overlays on the input image.\n",
    "\n",
    "For more information, check `visualize_segmentation_errors` and `visualize_segmentation_errors_sample` (for visualizing segmentation errors of just a sample) in `library`.\n",
    "\n",
    "> THE FUNCTIONS MUST BE ADAPTED TO THESE MULTI-CHANNEL IMAGES!!! TAKE `display_image` AS THE INSPIRATION. I have already made some changes that should make the function work, but I'm not 100% sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cce03d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a few random samples\n",
    "visualizations.visualize_segmentation_errors_sample(\n",
    "    images=test_images_sample,\n",
    "    true_masks=test_true_masks_sample,\n",
    "    predictions=test_pred_masks_sample,\n",
    "    num_classes=5,\n",
    "    num_samples=3,\n",
    "    rgb_bands=(3, 2, 1) # S2 RGB bands are B4, B3, B2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b02d17",
   "metadata": {},
   "source": [
    "## 2.5. Setting the optimal classification threshold\n",
    "\n",
    "Assuming that we give equal importance to false positives and false negatives, we set a decision probability threshold that strikes a good balance between the two.\n",
    "\n",
    "Since we want to give equal importance to reduce false positives (related to precision, as it is the ratio of positive instances correctly predicted over all positive predictions) and false negatives (related to recall, since it is the ratio of positive instances correctly predicted over all positive instances), **we should probably aim for a decision probability threshold which maximizes the F1-score, since it is the harmonic mean of precision and recall**, where:\n",
    "\n",
    "$$F1-score = \\frac{2\\times Precision \\times Recall}{Precision + Recall}$$\n",
    "\n",
    "To do that, it can be useful to start by plotting the precision-recall curve for different probability thresholds (just to visualize the trade-off when changing the probability threshold), and then search for the probability threshold which maximizes the F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61438534",
   "metadata": {},
   "source": [
    "### Precision-recall curve with F1-score for different probability thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d766cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "test_predictions_prob, test_labels = nn_model.predict(\n",
    "    model=model,\n",
    "    dataloader=test_loader,\n",
    "    device=device,\n",
    "    num_classes=5,  # Number of classes in the dataset (0-4 for none to high burn severity)\n",
    "    return_probs=True,  # Get probabilities instead of predictions of the most likely class\n",
    ")\n",
    "\n",
    "print(f\"Predictions shape: {test_predictions.shape}\")\n",
    "print(f\"Labels shape: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a2fb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third, we initialize the metrics and lists for storing the results\n",
    "precision_metric = classification.BinaryPrecision().to(device)\n",
    "recall_metric = classification.BinaryRecall().to(device)\n",
    "f1_metric = classification.BinaryF1Score().to(device)\n",
    "\n",
    "results = {\n",
    "    'thresholds': [],\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'f1_score': []\n",
    "}\n",
    "\n",
    "# Fourth, we calculate the metrics for different thresholds \n",
    "for threshold in np.arange(0, 1.01, 0.005):\n",
    "\n",
    "    # Move tensors to the same device as the metrics to avoid errors\n",
    "    test_predictions_device = test_predictions_prob.to(device)\n",
    "    test_labels_device = test_labels.to(device)\n",
    "\n",
    "    # Convert probabilities to binary predictions using the threshold\n",
    "    test_predictions_binary = (test_predictions_device > threshold).float()\n",
    "\n",
    "    # Calculate metrics (make sure inputs are tensors)\n",
    "    precision_torch = precision_metric(test_predictions_binary, test_labels_device)\n",
    "    recall_torch = recall_metric(test_predictions_binary, test_labels_device)\n",
    "    f1_torch = f1_metric(test_predictions_binary, test_labels_device)\n",
    "\n",
    "    # Store the results\n",
    "    results['thresholds'].append(threshold)\n",
    "    results['precision'].append(precision_torch.item())\n",
    "    results['recall'].append(recall_torch.item())\n",
    "    results['f1_score'].append(f1_torch.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34029340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we convert the results to a DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c670ec1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(results_df['recall'], results_df['precision'])\n",
    "plt.title('Precision-recall curve with F1-scores')\n",
    "\n",
    "# Add vertical and horizontal lines for precision and recall of maximum F1-score\n",
    "max_f1_index = results_df['f1_score'].idxmax()\n",
    "plt.scatter(results_df['recall'][max_f1_index], results_df['precision'][max_f1_index], \n",
    "            color='red', s=100, zorder=5, label=f'Best Threshold (F1 = {results_df['f1_score'][max_f1_index]:.2f})')\n",
    "\n",
    "# Adjust scale of the axes\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add grid, labels and legend\n",
    "plt.grid()\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1343a598",
   "metadata": {},
   "source": [
    "- The beginning of the curve represents the highest probability thresholds possible.\n",
    "- On the other hand, the end of the curve represents the lowest probability thresholds possible, where the recall converges to a high value (as the number of false negatives will be very low, as we will more easily classify an instance as positive).\n",
    "- The precision converges, at the end of the curve, to the class imbalance proportion, where we will classify all instances as positive. This happens because all the actual positives will be correctly predicted, but all the actual negatives will be incorrectly predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c061e6b5",
   "metadata": {},
   "source": [
    "### Optimal classification threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bd7027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best threshold in terms of the F1-score\n",
    "print('Best threshold and metrics for the maximum F1-score:')\n",
    "display(results_df.loc[results_df['f1_score'].idxmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8288ed92",
   "metadata": {},
   "source": [
    "# 3. Conclusions"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
